#!/bin/bash

# Set log file path
LOG_FILE="$HOME/discourse_backup.log"

# Telegram bot settings
BOT_TOKEN=""
CHAT_ID=""

# Function to send a message via Telegram
send_telegram_message() {
  local message=$1
  curl -s -X POST "https://api.telegram.org/bot$BOT_TOKEN/sendMessage" \
       -d chat_id="$CHAT_ID" \
       -d text="$message"
}

# Function to log messages and notify via Telegram
log_and_notify() {
  local message=$1
  echo "$(date): $message" >> $LOG_FILE
  send_telegram_message "$message"
}

# VPS connection settings
VPS_USER="root"
VPS_HOST=""
VPS_PASSWORD=""
VPS_BACKUP_DIR="/var/discourse/shared/standalone/backups/default"
LOCAL_BACKUP_DIR="$HOME/discourse_backups"
REMOTE_NAME="forumbackup:1SPw-YYAjD1-yloFfUPNT_eA_8Fc-5by2"  # Use folder ID

# Maximum retry attempts for download and upload
MAX_RETRIES=3

# Ensure local backup directory exists
mkdir -p $LOCAL_BACKUP_DIR

# Get the current timestamp
CURRENT_DAY=$(date +%s)

# Check if it's time to download a new backup
log_and_notify "Starting backup check process..."

# Get the latest backup on the VPS
LATEST_BACKUP=$(sshpass -p "$VPS_PASSWORD" ssh -o StrictHostKeyChecking=no $VPS_USER@$VPS_HOST "ls -t $VPS_BACKUP_DIR/*.tar.gz | head -n 1")

if [ -z "$LATEST_BACKUP" ]; then
    log_and_notify "Failed to retrieve the latest backup file name from VPS."
    exit 1
fi

LATEST_BACKUP_FILE=$(basename $LATEST_BACKUP)
log_and_notify "Latest backup on VPS: $LATEST_BACKUP_FILE"

# Check if the latest backup exists locally
if [ -f "$LOCAL_BACKUP_DIR/$LATEST_BACKUP_FILE" ]; then
    log_and_notify "Backup $LATEST_BACKUP_FILE already exists locally. No need to download."
else
    # Download the latest backup from VPS
    log_and_notify "Downloading the latest backup from VPS..."

    for i in $(seq 1 $MAX_RETRIES); do
        log_and_notify "Attempting to download backup... Attempt $i"

        sshpass -p "$VPS_PASSWORD" scp -o StrictHostKeyChecking=no $VPS_USER@$VPS_HOST:$VPS_BACKUP_DIR/$LATEST_BACKUP_FILE $LOCAL_BACKUP_DIR

        if [ $? -eq 0 ]; then
            log_and_notify "Backup $LATEST_BACKUP_FILE downloaded successfully to $LOCAL_BACKUP_DIR"
            break
        else
            log_and_notify "Download attempt $i failed."
            sleep 5
        fi

        if [ $i -eq $MAX_RETRIES ]; then
            log_and_notify "Failed to download backup after $MAX_RETRIES attempts."
            exit 1
        fi
    done
fi

# Upload the backup to Google Drive if it's not already there
log_and_notify "Checking if backup already exists on Google Drive..."
if rclone lsf $REMOTE_NAME | grep -q "$LATEST_BACKUP_FILE"; then
    log_and_notify "Backup $LATEST_BACKUP_FILE already exists on Google Drive. Skipping upload."
else
    for i in $(seq 1 $MAX_RETRIES); do
        log_and_notify "Attempting to upload backup to Google Drive... Attempt $i"

        rclone -vv copy $LOCAL_BACKUP_DIR/$LATEST_BACKUP_FILE $REMOTE_NAME

        if [ $? -eq 0 ]; then
            log_and_notify "Backup $LATEST_BACKUP_FILE uploaded successfully to Google Drive"
            break
        else
            log_and_notify "Upload attempt $i failed."
            sleep 5
        fi

        if [ $i -eq $MAX_RETRIES ]; then
            log_and_notify "Failed to upload backup after $MAX_RETRIES attempts."
            exit 1
        fi
    done
fi

# Cleanup local backups, retaining only the most recent file
log_and_notify "Cleaning up local backups, retaining only the most recent one..."
cd $LOCAL_BACKUP_DIR
ls -t | tail -n +2 | xargs rm -f
log_and_notify "Local cleanup completed. Only the most recent backup is retained locally."

# Cleanup Google Drive backups, retaining only the most recent file
log_and_notify "Cleaning up older backups on Google Drive..."
rclone lsf $REMOTE_NAME --format "t" | sort -r | tail -n +2 | while read -r file; do
    rclone delete $REMOTE_NAME/"$file"
    log_and_notify "Deleted older backup on Google Drive: $file"
done
log_and_notify "Google Drive cleanup completed. Only the most recent backup is retained on Google Drive."

log_and_notify "Backup process completed."
